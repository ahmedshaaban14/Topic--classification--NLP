# -*- coding: utf-8 -*-
"""NLP Topic Modelling Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eaOz7KgGXQ3Rm3IOPZJNdr8Fiagq2CD6
"""

import copy
import re
import matplotlib as matplotlib
import nltk
import numpy as np
from matplotlib import pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pandas as pd
from sklearn.manifold import MDS
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import seaborn as sns

# from google.colab import drive
# drive.mount('/content/drive')

# nltk.download('averaged_perceptron_tagger')
# nltk.download('wordnet')
# nltk.download("stopwords")
# nltk.download('punkt')
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np
import seaborn as sns

"""Our model class"""

df = pd.read_csv("drive/MyDrive/articles1.csv")
documents = df.sample(1000, random_state=11)
original_words = dict()


class TopicModelerLDA:
    def __init__(self, num_topics):
        self.num_topics = num_topics
        self.vectorizer = TfidfVectorizer(min_df=0.3, max_df=0.6)
        self.lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=0)
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = SnowballStemmer('english')
        self.stop_words = set(stopwords.words('english'))
        self.stop_words.update(
            ['can', 'may', 'must', 'shall', 'will', 'could', 'might', 'should', 'would', 'mr', 'new', 'said', 'like',
             'one'])
        self.cluster_names = {
            0: 'Topic 0',
            1: 'Topic 1',
            2: 'Topic 2',
            3: 'Topic 3'
        }

    def _preprocess(self, documents):
        documents = self._doc_to_list(documents)
        documents_preprocessed = []
        for doc in documents:
            words = [word for word in doc.lower().split() if
                     word not in self.stop_words and re.match(r'^[a-zA-Z]+$', word)]
            lemmas = [self.lemmatizer.lemmatize(word) for word in words]
            stems = [self.stemmer.stem(lemma) for lemma in lemmas]
            for i in range(len(stems)):
                original_words[stems[i]] = words[i]
            preprocessed_doc = ' '.join(stems)
            documents_preprocessed.append(preprocessed_doc)
        return documents_preprocessed

    def _doc_to_list(self, documents):
        documents = documents['content'].tolist()
        punc = "[]?!();:\"'=*/|&%{}@§_$“”<>-—,.\'’"
        documents = [''.join(char for char in word if char not in punc) for word in documents]
        return documents

    def fit(self, documents):
        preprocessed = self._preprocess(documents)
        tfidf_matrix = self.vectorizer.fit_transform(preprocessed)
        self.lda_model.fit(tfidf_matrix)

    def print_top_words_for_topics(self):
        feature_names = self.vectorizer.get_feature_names_out()
        for topic_idx, topic in enumerate(self.lda_model.components_):
            top_words_indices = np.argsort(topic)[::-1][:5]
            top_words = [original_words[feature_names[i]] for i in top_words_indices]
            print(f"Topic {topic_idx} words: {' '.join(top_words)}")

    def show_topics_plot(self):
        topics = self.lda_model.transform(self.vectorizer.transform(self._preprocess(documents)))
        topic_labels = np.argmax(topics, axis=1)
        sns.set_palette("husl", self.num_topics)
        sns.scatterplot(x=topics[:, 0], y=topics[:, 1], hue=topic_labels, legend='full', palette="deep")
        plt.xlabel("Topic 1")
        plt.ylabel("Topic 2")
        plt.title("Topic Clusters")
        plt.show()


print("=====================================\nLDA Model : \n=====================================")

# Instantiate the TopicModelerLDA with the desired number of topics
model = TopicModelerLDA(num_topics=4)

# Fit the model with the documents
model.fit(documents)

# Print the top words for each topic
model.print_top_words_for_topics()

model.show_topics_plot()

print("=====================================\nK-means Model : \n=====================================")

df = pd.read_csv("drive/MyDrive/articles1.csv")
documents = df.sample(300, random_state=10)

"""Our model class"""


class TopicModdeler():
    def __init__(self, num_clusters):
        self.num_clusters = num_clusters
        self.vectorizer = TfidfVectorizer()
        self.KMeanModel = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=1, random_state=0)
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = SnowballStemmer('english')
        self.stop_words = set(stopwords.words('english'))
        self.stop_words.update(['can',
                                'may',
                                'must',
                                'shall',
                                'will',
                                'could',
                                'might',
                                'should',
                                'would',
                                'mr',
                                'new',
                                'said',
                                'like',
                                'one'])
        self.cluster_names = {0: 'Social',
                              1: 'Economics',
                              2: 'Crime/Action',
                              3: 'Politics'
                              }

    def _tokenize_and_stem(self, text):
        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
        filtered_tokens = []
        for token in tokens:
            if re.search('[a-zA-Z]', token):
                filtered_tokens.append(token)
        stems = [self.stemmer.stem(t) for t in filtered_tokens]
        return filtered_tokens, stems

    def _doc_to_list(self, documents):
        documents = documents['content'].tolist()
        punc = "[]?!();:\"'=*/|&%{}@§_$“”<>-—,.\'’"
        documents = [''.join(char for char in word if char not in punc) for word in documents]
        return documents

    def _preprocess(self, documents):
        documents = self._doc_to_list(documents)
        documents_preprocessed = []
        for doc in documents:
            words = [word for word in doc.lower().split() if
                     word not in self.stop_words and re.match(r'^[a-zA-Z]+$', word)]
            lemmas = [self.lemmatizer.lemmatize(word) for word in words]
            stems = [self.stemmer.stem(lemma) for lemma in lemmas]
            preprocessed_doc = ' '.join(stems)
            documents_preprocessed.append(preprocessed_doc)
        return documents_preprocessed

    def fit(self, documents):
        preprocessed = self._preprocess(documents)
        tfidf_matrix = self.vectorizer.fit_transform(preprocessed)
        self.KMeanModel.fit(tfidf_matrix)
        self.similarity_distance = 1 - cosine_similarity(tfidf_matrix)

    def printTopWordsForClusters(self, documents):
        documents = self._doc_to_list(documents)

        clusters = self.KMeanModel.labels_.tolist()
        news_data = {'documents': documents, 'cluster': clusters}
        frame = pd.DataFrame(news_data, index=[clusters], columns=['cluster'])

        totalvocab_stemmed = []
        totalvocab_tokenized = []
        for i in documents:
            allwords_tokenized, allwords_stemmed = self._tokenize_and_stem(i)
            totalvocab_stemmed.extend(allwords_stemmed)
            totalvocab_tokenized.extend(allwords_tokenized)

        vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)

        # sort cluster centers by proximity to centroid
        order_centroids = self.KMeanModel.cluster_centers_.argsort()[:, ::-1]
        terms = self.vectorizer.get_feature_names_out()
        for i in range(self.num_clusters):
            print("Cluster %d words:" % i, end='')
            for ind in order_centroids[i, :6]:
                print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0],
                      end='')
            print()

    def show_clusters_plot(self):
        clusters = self.KMeanModel.labels_.tolist()
        # Convert two components as we're plotting points in a two-dimensional plane
        mds = MDS(n_components=2, dissimilarity="precomputed", random_state=1)
        pos = mds.fit_transform(self.similarity_distance)  # shape (n_components, n_samples)
        xs, ys = pos[:, 0], pos[:, 1]

        # Set up colors per clusters using a dict
        cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a'}

        # Create data frame that has the result of the MDS and the cluster
        dff = pd.DataFrame(dict(x=xs, y=ys, label=clusters))
        groups = dff.groupby('label')

        # Set up plot
        fig, ax = plt.subplots(figsize=(17, 9))  # set size

        for name, group in groups:
            ax.plot(group.x, group.y, marker='o', linestyle='', ms=20,
                    label=self.cluster_names[name], color=cluster_colors[name],
                    mec='none')
            ax.set_aspect('auto')
            ax.tick_params( \
                axis='x',
                which='both',
                bottom='off',
                top='off',
                labelbottom='off')
            ax.tick_params( \
                axis='y',
                which='both',
                left='off',
                top='off',
                labelleft='off')

        ax.legend(numpoints=1)
        plt.show()

    def predict(self, docs):
        preprocess = self._preprocess(docs)
        test_tfidf_matrix = self.vectorizer.transform(preprocess)
        test_pred = self.KMeanModel.predict(test_tfidf_matrix)
        docs['Cluster'] = [self.cluster_names[clusterIndex] for clusterIndex in test_pred]
        result = docs[['id', 'title', 'Cluster']]
        print(result)


"""To know the suitable number of Clusters"""

# # Code to show elbow of K-Means
# from sklearn.cluster import KMeans
# from yellowbrick.cluster import KElbowVisualizer
# from sklearn.datasets import make_blobs

# # Run KMeans with the elbow method
# model = KMeans()
# visualizer = KElbowVisualizer(model, k=(1,11))
# import numpy as np
# visualizer.fit(tfidf_matrix.toarray())
# visualizer.show()

"""Training"""

model = TopicModdeler(num_clusters=4)
model.fit(documents)

"""Prediction"""

test_data = df[df['id'].isin(range(17283, 17300))]
model.predict(test_data)

"""Visualization"""

model.printTopWordsForClusters(documents)
model.show_clusters_plot()